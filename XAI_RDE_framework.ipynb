{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ad0bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 meilleures prédictions pour l'image :\n",
      "  1. Egyptian_cat : 0.3862\n",
      "  2. tabby : 0.2892\n",
      "  3. tiger_cat : 0.1696\n",
      "  4. printer : 0.0292\n",
      "  5. hamper : 0.0179\n",
      "\n",
      "Prédiction principale (utilisée pour RDE) : Egyptian_cat (Classe 285) avec probabilité 0.3862\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Configuration et Chargement du Modèle ---\n",
    "\n",
    "# Charger le modèle ResNet50 pré-entraîné sur ImageNet\n",
    "model = tf.keras.applications.ResNet50(weights='imagenet', include_top=True)\n",
    "model.trainable = False # Geler le modèle, nous n'optimisons que le masque\n",
    "\n",
    "# Définir la taille d'entrée attendue par ResNet50\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# --- 2. Chargement et Préparation de l'Image (x) ---\n",
    "\n",
    "# Télécharger une image d'exemple\n",
    "img_url = \"data/chat_roux.jpg\"\n",
    "\n",
    "# Charger l'image et la redimensionner\n",
    "img = tf.keras.utils.load_img(img_url, target_size=IMG_SIZE)\n",
    "# 'x' est notre image de base (0-255), de forme (224, 224, 3)\n",
    "x = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "# Fonction pour pré-traiter l'image pour ResNet50\n",
    "def preprocess_image(img_tensor):\n",
    "    # Ajoute la dimension batch et applique le pré-traitement ResNet\n",
    "    return tf.keras.applications.resnet50.preprocess_input(img_tensor[tf.newaxis, ...])\n",
    "\n",
    "# --- 3. Obtenir la Prédiction Originale (Phi(x)) ---\n",
    "\n",
    "# Pré-traiter l'image originale\n",
    "x_preprocessed = preprocess_image(x)\n",
    "\n",
    "# Obtenir les prédictions (logits)\n",
    "original_preds = model(x_preprocessed, training=False)\n",
    "\n",
    "# Appliquer Softmax pour obtenir les probabilités [cite: 161]\n",
    "original_probs = tf.nn.softmax(original_preds[0])\n",
    "\n",
    "# Obtenir la classe prédite (j*) et sa probabilité\n",
    "j_star = tf.argmax(original_probs)\n",
    "\n",
    "# Afficher les 5 prédictions\n",
    "decoded_preds = tf.keras.applications.resnet50.decode_predictions(original_preds.numpy(), top=5)\n",
    "original_prob_value = decoded_preds[0][0][2]\n",
    "print(\"Les 5 meilleures prédictions pour l'image :\")\n",
    "for i in range(5):\n",
    "    class_name = decoded_preds[0][i][1]\n",
    "    prob = decoded_preds[0][i][2]\n",
    "    print(f\"  {i+1}. {class_name} : {prob:.4f}\")\n",
    "\n",
    "# Afficher la prédiction principale qui sera utilisée pour RDE\n",
    "print(f\"\\nPrédiction principale (utilisée pour RDE) : {decoded_preds[0][0][1]} (Classe {j_star}) avec probabilité {original_prob_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e47b1fac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AbortedError",
     "evalue": "{{function_node __wrapped____MklNativeConv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Operation received an exception:Status: 1, message: could not create a memory object, in file tensorflow/core/kernels/mkl/mkl_conv_grad_input_ops.cc:546 [Op:Conv2DBackpropInput] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAbortedError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     49\u001b[39m     total_loss = D + R\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 5. Rétropropagation pour mettre à jour le masque 's'\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Calcule les gradients de la perte par rapport à 's'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m gradients = \u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Applique les gradients à 's'\u001b[39;00m\n\u001b[32m     56\u001b[39m optimizer.apply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, [s]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tensorflow\\python\\eager\\backprop.py:1066\u001b[39m, in \u001b[36mGradientTape.gradient\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1060\u001b[39m   output_gradients = (\n\u001b[32m   1061\u001b[39m       composite_tensor_gradient.get_flat_tensors_for_gradients(\n\u001b[32m   1062\u001b[39m           output_gradients))\n\u001b[32m   1063\u001b[39m   output_gradients = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops.convert_to_tensor(x)\n\u001b[32m   1064\u001b[39m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m flat_grad = \u001b[43mimperative_grad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent:\n\u001b[32m   1075\u001b[39m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[32m   1076\u001b[39m   \u001b[38;5;28mself\u001b[39m._watched_variables = \u001b[38;5;28mself\u001b[39m._tape.watched_variables()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[39m, in \u001b[36mimperative_grad\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m     64\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     65\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % unconnected_gradients)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tensorflow\\python\\eager\\backprop.py:148\u001b[39m, in \u001b[36m_gradient_function\u001b[39m\u001b[34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[39m\n\u001b[32m    146\u001b[39m     gradient_name_scope += forward_pass_name_scope + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(gradient_name_scope):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:584\u001b[39m, in \u001b[36m_Conv2DGrad\u001b[39m\u001b[34m(op, grad)\u001b[39m\n\u001b[32m    575\u001b[39m shape_0, shape_1 = array_ops.shape_n([op.inputs[\u001b[32m0\u001b[39m], op.inputs[\u001b[32m1\u001b[39m]])\n\u001b[32m    577\u001b[39m \u001b[38;5;66;03m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# functions for performance reasons in Eager mode. gen_nn_ops functions take a\u001b[39;00m\n\u001b[32m    579\u001b[39m \u001b[38;5;66;03m# `explicit_paddings` parameter, but nn_ops functions do not. So if we were\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# in Eager mode.\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[43mgen_nn_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d_backprop_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshape_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdilations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    594\u001b[39m     gen_nn_ops.conv2d_backprop_filter(\n\u001b[32m    595\u001b[39m         op.inputs[\u001b[32m0\u001b[39m],\n\u001b[32m    596\u001b[39m         shape_1,\n\u001b[32m    597\u001b[39m         grad,\n\u001b[32m    598\u001b[39m         dilations=dilations,\n\u001b[32m    599\u001b[39m         strides=strides,\n\u001b[32m    600\u001b[39m         padding=padding,\n\u001b[32m    601\u001b[39m         explicit_paddings=explicit_paddings,\n\u001b[32m    602\u001b[39m         use_cudnn_on_gpu=use_cudnn_on_gpu,\n\u001b[32m    603\u001b[39m         data_format=data_format)\n\u001b[32m    604\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:1866\u001b[39m, in \u001b[36mconv2d_backprop_input\u001b[39m\u001b[34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[39m\n\u001b[32m   1864\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   1865\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1866\u001b[39m   \u001b[43m_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1867\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   1868\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tensorflow\\python\\framework\\ops.py:6027\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6025\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6026\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6027\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAbortedError\u001b[39m: {{function_node __wrapped____MklNativeConv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:CPU:0}} Operation received an exception:Status: 1, message: could not create a memory object, in file tensorflow/core/kernels/mkl/mkl_conv_grad_input_ops.cc:546 [Op:Conv2DBackpropInput] name: "
     ]
    }
   ],
   "source": [
    "# Hyperparamètres\n",
    "lambda_val = 0.6        # \n",
    "num_steps = 2000        # \n",
    "num_samples = 64        # [cite: 239]\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.003) # \n",
    "\n",
    "# Le masque entraînable\n",
    "s = tf.Variable(tf.ones_like(x), trainable=True)\n",
    "\n",
    "# Moyenne et écart-type pour le bruit v \n",
    "mu = tf.reduce_mean(x)\n",
    "sigma = tf.math.reduce_std(x)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. Garder le masque s dans [0, 1]\n",
    "        # (Nous le faisons via assign pour que la variable 's' soit mise à jour)\n",
    "        s.assign(tf.clip_by_value(s, 0, 1)) \n",
    "        \n",
    "        # 2. Estimer l'espérance de la distorsion (Monte-Carlo)\n",
    "        distortion = 0.0\n",
    "        for _ in range(num_samples):\n",
    "            # Générer le bruit v [cite: 146, 236]\n",
    "            v = tf.random.normal(shape=x.shape, mean=mu, stddev=sigma, dtype=tf.float32)\n",
    "            \n",
    "            # Créer l'entrée perturbée y\n",
    "            y = x * s + (1 - s) * v\n",
    "            \n",
    "            # Obtenir la prédiction perturbée Phi(y)\n",
    "            # Ajout de la dimension batch (batch_size=1)\n",
    "            y_batch = y[tf.newaxis, ...] \n",
    "            perturbed_output = model(y_batch, training=False) # training=False est important\n",
    "            \n",
    "            # Obtenir le score pour la classe d'origine j*\n",
    "            perturbed_prob = perturbed_output[0, j_star]\n",
    "            \n",
    "            # Calculer la distorsion d (ex: d_2 )\n",
    "            # L'article multiplie par 100, mais c'est juste une mise à l'échelle\n",
    "            distortion += tf.square(original_prob_value - perturbed_prob)\n",
    "            \n",
    "        # Moyenne de la distorsion\n",
    "        D = distortion / num_samples\n",
    "        \n",
    "        # 3. Calculer la pénalité de parcimonie (R) [cite: 24]\n",
    "        R = lambda_val * tf.norm(s, ord=1) # Norme L1\n",
    "        \n",
    "        # 4. Perte totale\n",
    "        total_loss = D + R\n",
    "\n",
    "    # 5. Rétropropagation pour mettre à jour le masque 's'\n",
    "    # Calcule les gradients de la perte par rapport à 's'\n",
    "    gradients = tape.gradient(total_loss, [s])\n",
    "    \n",
    "    # Applique les gradients à 's'\n",
    "    optimizer.apply_gradients(zip(gradients, [s]))\n",
    "\n",
    "# 6. Le résultat final est le masque 's' optimisé\n",
    "final_explanation_mask = s.numpy() # Convertit en array NumPy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
